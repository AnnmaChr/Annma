function plotData(X, y)
%PLOTDATA Plots the data points X and y into a new figure 
%   PLOTDATA(x,y) plots the data points with + for the positive examples
%   and o for the negative examples. X is assumed to be a Mx2 matrix.



% ====================== YOUR CODE HERE ======================
% Instructions: Plot the positive and negative examples on a
%               2D plot, using the option 'k+' for the positive
%               examples and 'ko' for the negative examples.
%



% =========================================================================

% data = load ('ex2data1.txt');
data = load ('ex2data1.txt');
X = data (:,[1, 2]); y = data(:,[3]);
  %Plotting of positive and negative examples
  pos = find(y==1); %index of positive results
  neg = find(y==0); %index of negative results
  
  % Create New Figure
  figure;
  %Plot Examples
  %Plot Positive Results 
  %    X_axis: Exam1 Score =  X(pos,1)
  %    Y_axis: Exam2 Score =  X(pos,2)
  plot (X(pos, 1), X(pos,2), 'k+', 'Linewidth', 2, 'Markersize', 7);
 
  hold on;  
  
  %Plot Negative Results on 
  %    X_axis: Exam1 Score =  X(neg,1)
  %    Y_axis: Exam2 Score =  X(neg,2)
  plot(X(neg,1),X(neg,2),'ko', 'MarkerFacecolor', 'y', 'Markersize', 7);

hold off;

end

function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

% Initialize some useful values
m = length(y); % number of training examples



% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta
%
% Note: grad should have the same dimensions as theta
%
h_theta = sigmoid(X*theta);
J = (1 / m) * ((-y' * log(h_theta)) - (1 - y)' * log(1 - h_theta));

grad = (1 / m) * (h_theta - y)' * X;





% =============================================================

end

function p = predict(theta, X)
%PREDICT Predict whether the label is 0 or 1 using learned logistic 
%regression parameters theta
%   p = PREDICT(theta, X) computes the predictions for X using a 
%   threshold at 0.5 (i.e., if sigmoid(theta'*x) >= 0.5, predict 1)


m = size(X, 1); % Number of training examples



% ====================== YOUR CODE HERE ======================
% Instructions: Complete the following code to make predictions using
%               your learned logistic regression parameters. 
%               You should set p to a vector of 0's and 1's
% % Dimentions:
  % X     =  m x (n+1)
  % theta = (n+1) x 1
  
  h_x = sigmoid(X*theta);
  p=(h_x>=0.5);
  
  %p = double(sigmoid(X * theta)>=0.5);





% =========================================================================


end

function [J, grad] = costFunctionReg(theta, x, y, lambda)
%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization
%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using
%   theta as the parameter for regularized logistic regression and the
%   gradient of the cost w.r.t. to the parameters. 



% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta
m = length(y);
h_theta = sigmoid(x*theta);  
J = ( (1/m)*sum(-y'*log(h_theta)-(1-y)'*log(1-h_theta)) )+(lambda/(2*m))*sum(theta(2:length(theta)).*theta(2:length(theta)));
%for j =0
grad = (1./m)*sum(x.*repmat((sigmoid(x*theta)-y), 1, size(x,2)));
%for j >1
grad(:,2:length(grad))=grad(:,2:length(grad))+(lambda/m)*theta(2:length(theta))';



  
% =============================================================

end
  
